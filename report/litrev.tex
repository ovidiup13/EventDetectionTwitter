%% ================================================================================
%% This LaTeX file was created by AbiWord.                                         
%% AbiWord is a free, Open Source word processor.                                  
%% More information about AbiWord is available at http://www.abisource.com/        
%% ================================================================================

\documentclass[a4paper,portrait,12pt]{article}
\usepackage[latin1]{inputenc}
\usepackage{calc}
\usepackage{setspace}
\usepackage{fixltx2e}
\usepackage{graphicx}
\usepackage{multicol}
\usepackage{minted}
\usepackage{listings}
\usepackage{algorithmicx}
\usepackage{algpseudocode}
% \usepackage{csquotes}
%\usepackage[normalem]{ulem}
%% Please revise the following command, if your babel
%% package does not support en-US
%\usepackage{babel}
\usepackage{color}
\usepackage{hyperref}
 
\begin{document}

\setlength{\oddsidemargin}{0.9847in-1in}
\setlength{\textwidth}{\paperwidth - 0.9847in-0.9847in}

\author{Ovidiu Popoviciu, 2036725}
\date{17th November 2017}
\title{Twitter Event Detection - MSci}
\maketitle

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\section{Introduction}

Event detection in social media is a challenging task.
This is due to the amount of tweets (approx. 500 million tweets per day in 2016 \cite{tweetStats}), amount of relevant tweets is very small (90\% are junk), limitations with the API (until recently \cite{twitterPremium}), importance of events and scalability issues, spelling and grammar mistakes in tweet body as well abbreviations and acronyms are difficult to identify.\\
The objective of this work is to evaluate a stage of the event detection process initially defined by McMinn et al. \cite{McMinn2013} \cite{McMinn2015} on a collection of tweets.

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\section{Approach}
\cite{McMinn2013} \cite{McMinn2015}

Two approaches are defined, each executed after the clustering stage provided by McMinn et al. \cite{McMinn2013}.

Following the work of McMinn et al. \cite{McMinn2013}, the steps taken in the processing of the dataset are:
\begin{itemize}
	\item Gathering data. This was already done for us, and the dataset was shared from the previous study.
	\item Preprocessing. The dataset was already pre-processed. Tweets were parsed, tagged and filtered. All that remained were the relevant tweets with tokens that might relate back to an event.
	\item Clustering. The data set contained clusters of tweets already defined, through the use of an Inverted Index and tweets grouped together using weight cosine similarity score.
	\item Burst Detection. Check for temporal bursts of a named entity in a window period of minutes to hours. This step was completed in conjunction with event merging.
	\item Cluster Identification. In McMinn et al. \cite{McMinn2013} evaluation program, clusters are already considered to map to a specific event since clusters with no named entities have been removed.
	\item Event Merging. Merging events that have multiple named entities. Discussion of this work is focused on this step.
\end{itemize}

The dataset provided has been pre-processed and organised into clusters.
The missing stages, which is provided in this work, is an enhancement to cluster identification and a different take on event merging.
The two approaches discussed here are:
\begin{enumerate}
	\item Filtering.\\
	      \\
	      The goal was to identify clusters which have limited or no relevance to any important events and remove them.\\
	      \\
	      This step may be considered as an enhancement of Cluster Identification.
	      More specifically, the objective was to see whether removing clusters with a particular number of tweets might reduce the resulting data set returned, resulting in an increased precision of the system. \\
	      \\
	      The filtering step is simple, as it consists of counting the number of tweets in each cluster, and dropping clusters with a size less than a number \textit{N}.
	      In this work, \textit{N} takes values from [5, 25].
	\item Merging. \\
	      \\
	      The goal was to study whether clusters with the same named entity, lying within the same temporal burst can be merged in the same cluster. \\
	      \\
		  In McMinn's study, clustering of tweets was optimised performance wise by reducing the number of tweets retrieved from the Inverted Index and, thus, reducing the number of comparisons for each tweet.
	      Thus, the initil objective was to check whether multiple clusters with the same event (and same named entity) have been created within a specific time period and if merging of these clusters would have a positive effect on the performance of the system.\\
	      \\
	      Clusters with the same named identity residing in the same temporal window are merged.
	      The merge action is simply the addition of all tweets from the selected cluster to its named entity match.
	      Comparison of window temporality is executed using the difference between the centroid times of two clusters, where the centroid time is the mean of all the tweets in a cluster.\\

	      The computationally expensive section of this approach is that the previous array of clusters must be kept sorted by time.
	      For each processed cluster, the first cluster to compare it against must be the most \"recent\" in order for this approach to work.
	      Thus, every time a new cluster gets added to the \textit{previousCluster} array or the centroid time of a cluster is updated, then the array must be re-sorted.\\
	      Fortunately, since only one cluster is updated at a time, the sort operation is computationally inexpensive (\textit{O}(\textit{N}) on nearly sorted array, using Bubble Sort.)

	      The window was tested with values (in minutes): 7, 15, 30, 60, 120 and 240.
	      \\\\
	      Pseudocode:
	      \begin{algorithmic}
		      \ForAll{$cluster$ of $clusters$}
		      \ForAll{$previous$ of $previousClusters$}
		      \If{$cluster.timestamp - previous.timestamp\geq window$}
		      \State $previousClusters.add(cluster)$
		      \State $break$
		      \EndIf
		      \If{$cluster.entity = previous.entity$}
		      \State $previous.addTweets(cluster.tweets)$
		      \State $break$
		      \EndIf
		      \EndFor
		      \State $previousClusters.sort()$
		      \EndFor
	      \end{algorithmic}
\end{enumerate}

The evaluation and discussion of the two approaches are dicusses in Section \ref{section-eval} and Section \ref{section-discussion}.

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\section{Code Description}
The implementation of the techniques were written in Java, due its object-oriented structure, efficiency and ability to handle 64-bit Integer values available from the data set.

\begin{figure}[h!]
	\centering
	\includegraphics[width=0.5\linewidth]{images/modelUML.png}
	\caption{UML model classes}
	\label{fig:modelUML}
\end{figure}

Initially, two model classes have been defined, as seen in Figure \ref{fig:modelUML}.
The first class, Tweet, contains the appropriate fields for identifying a tweet, such as: tweet ID, timestamp, user ID, parsed tweet tokens and the body text of the tweet.
Additionally, a second model class defined is Cluster, which references the previous class tweet. Cluster class contains a list of tweets with the same properties and is defined by a cluster ID, by the named entity which are identified by all the tweets and a timestamp which, we'll see in the next paragraphs, will hold the centroid time of all tweets in that specific cluster.\\

The data set is read from local disk through an instance of IOProcessor, which is an interface for reading data.
Since the data is in CSV format, a class CSVProcessor implementing the IOProcessor interface has been defined.
It provides methods for reading CSV cluster files and instantiating model classes to be used in the logic of the system.
Diagram for Processor classes can be seen in Figure \ref{fig:processorUML}. \\

\begin{figure}[h!]
	\centering
	\includegraphics[width=0.5\linewidth]{images/processorUML.png}
	\caption{UML IO processor classes}
	\label{fig:processorUML}
\end{figure}

Data is read and processed as a stream of Cluster objects on which multiple sequential operations can be applied.
ClusterFilter is an interface that can be extended to add more available operations.
The available operations, as seen in \ref{fig:filterUML}, are:
\begin{itemize}
	\item NumberOfTweetsFilter - is a class that filters the cluster by a set number of tweets.
	      A variable containing the number of tweets to filter by can be passed to the constructor of this class.
	\item MergeNamedEntityFilter - is a filter that merges clusters with the same named entities.
		  A variable denoting the time window can be initialized from the constructor of this class.
		  To speed up the performance of the filter, a map indexed by entities to list of clusters is used.
		  Every time a new cluster must be processed, the list of clusters with the same entity is retrieved from the map. 
		  This approach reduces the amount of clusters to compare to however, through creating a more complex data structure with the map of arrays.
		  In short, this approach trades off space complexity for time complexity. 

\end{itemize}

\begin{figure}[h!]
	\centering
	\includegraphics[width=0.7\linewidth]{images/filterUML.png}
	\caption{UML filter classes}
	\label{fig:filterUML}
\end{figure}

Each one of these filters can be applied in any sequence and the entire code can be run into a class containing the main function.
The Main class contains references to the other model, processor and filter classes and contains the entire flow of the system.
For the two approaches, two main classes have been defined: \textit{MergeNamedEntities.java} and \textit{FilterByNumberOfTweets.java}.
The classes read data from IOProcessor, apply the filtering and merging operations on the stream of data and write the resulting clusters back to a new CSV file.

\begin{figure}[h!]
	\centering
	\includegraphics[width=0.7\linewidth]{images/mainUML.png}
	\caption{UML Main class}
	\label{fig:mainUML}
\end{figure}

The evaluation stage was the same as McMinn et al. \cite{McMinn2013}, which used a python script to evaluate the CSV files containing the clusters against crowd gathered events.
Additionally, due to the repetitive nature of the approaches, evaluating each CSV cluster file, gathering of results and manually pasting them into a data processor (e.g. Excel) may take a long time.
For this problem, a Gradle tasks was implemented that automatically run the two Java programs and generate all the files using only one command.
To gather the results from the output of the Python evaluation program, a NodeJS script has been developed that runs the evaluation automatically and creates a JSON containing the respective results for the analysis.
For example, all the results for the 1 day dataset with the filtering approach applied, would be contained within a single JSON file 1day-filtered.json.
The JSON schema is as follows:

\begin{lstlisting}[caption=JSON Schema Example, label=json-schema]
[{
    "file": "0",
    "data": {
        "events": {
            "total": 506,
            "detected": 19
        },
        "clusters": {
            "total": 8829,
            "matched": 120
        },
        "stats": {
            "recall": 0.038,
            "precision": 0.014,
            "fmeasure": 0.02
        }
    }
},
...
]
\end{lstlisting}

The schema contains a number of properties, of which the most important are:
\begin{itemize}
	\item \textit{file} - this denotes the file that was used for evaluation.
	      For example, in the case of filtering approach, a file with the name \textit{8.csv} denotes that a filtering of a minimum number of 8 tweets has been applied to the initial dataset.
	\item \textit{data} - this property contains an object of the statistics of the evaluation.
\end{itemize}

The rest of the properties are self-descriptive.\\

Once all files have been evaluated, the JSON schemas are loaded into a JavaScript browser file.
This JavaScript file contains logic that constructs visualizations of the evaluation results.
The charting functionality is provided by the frappe-charts package \cite{frappeCharts}.\\

The source code is available in the \textit{source.zip} file as well as instructions on how to run the code contained in the \textit{README.md} file.

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\section{Evaluation}
\label{section-eval}

The evaluation of the two approaches relies on a few measures:
\begin{itemize}
	\item \textbf{Precision}.
	      This measure signifies the proportion of the clusters that are relevant out of the returned set of events.
	\item \textbf{Recall}.
	      Defines the fraction of clusters that were found to be relevant out of the entire set of relevant events.
	      This measure, in theory, cannot increase further than its baseline value, since it is largely dependent on the clustering step.
	\item \textbf{F-Measure}.
	      It is an overall score of performance, and is calculated as the weighted harmonic mean of the precision and recall measures.
\end{itemize}

Since recall precision cannot increase, then the evaluation focuses on increased precision with minimal negative impact to recall.\\
\\
Initially, the two baseline CSV files containing 1 day and 7 days data have been evaluated.
In the following figures, the element \textbf{0} on the \textit{x}-axis, is considered the baseline result.
The rest of the elements, can be considered in the context of either filtering or merging.
For example, the element \textbf{8} in the context of filtering, refers to the clusters which have been filtered by a minimum of 8 tweets.
Additionally, the element \textbf{30} in the context of merging, refers to the window value of 30 minutes.

A summary of the results, based on the f-measure for all sets can be seen in \ref{fig:summary}.
Additionally, the full charts of the experiment can be viewed in Appendix \ref{appendix:results}.

\begin{enumerate}
	\item \textbf{Filtering}\\
	\\
	      Filtering by number of tweets has a massive effect on the results of the evaluation. \\
	      Applying the filtering process on the 1-day data set, increased the precision of by a magnitude while the recall measure remained constant.
	      The change in precision may be due to junk clusters returned that have no relevance to the results.
	      \begin{figure}[h!]
		      \centering
		      \includegraphics[width=\linewidth]{images/1day-filtered-f-measure.png}
		      \caption{1 Day Filtered - F-Measure}
		      \label{fig:1day-filtered-f-measure}
	      \end{figure}

	      As mentioned before, \textit{N} was tested within a range of [5, 25] and the results can be seen in Figure \ref{fig:1day-filtered-f-measure}.
	      The overall f-measure jumped from the initial evaluation and slowly increased up until \textit{N = 10}.
	      The f-measure slowly decreased thereafter, due to the number of relevant clusters being slowly filtered out.
	      This negatively impacted the recall measurement, from a base measure of 0.038 to 0.28.
	      On the other hand, precision increased dramatically as the number of clusters on each experiment decreased.
	      \\
	      Finally, for the 1-day data set, the optimal number of tweets in a cluster was 10.
	      This provided the highest f-measure of the entire range [5, 25] and with recall remaining constant.

	      Applying the same process on the 7-day data set, similar results emerged.
	      The overall f-measure jumped from the initial value of 0.026 to a maximum value of 0.242, for \textit{N = 11}.
	      Precision increased with incremental of \textit{N} while recall had slowly decreased, but with no major impact.
	      The trend of the f-measure for the 7-day data set can be seen in Figure \ref{fig:7days-filtered-f-measure}.
	      \begin{figure}[h!]
		      \centering
		      \includegraphics[width=\linewidth]{images/7days-filtered-f-measure.png}
		      \caption{7 Days Filtered - F-Measure}
		      \label{fig:7days-filtered-f-measure}
	      \end{figure}

	\item \textbf{Merging}\\
	\\
		  By contrast with the filtering approach, merging step gave surprising results.
		  For the 1-day dataset, the overall f-measure is lower than the filtering approach, due to precision and the amount of clusters still remaining, as seen in Figure \ref{fig:1day-merged-f-measure}.
		  \\\\
		  Although the total clusters have been reduced by up to 50\% compared to the base line, there still remain thousands of clusters compared to the filtering approach where this number has been reduced to less than 500.
		  Nevertheless, f-measure has increased over the baseline measure, from 0.020 to 0.024 at its highest.  
		  Precision has slowly increased along with the time window, since there were more clusters with the same named entity indentified in a larger temporal space.
		  However, the surprising measure to have increased was recall from 0.038 baseline to 0.04.
		  Moreover, the number of events detected has increased from initial baseline 19, to 20. 
	      \begin{figure}[h!]
		      \centering 
		      \includegraphics[width=\linewidth]{images/1day-merged-f-measure.png}
		      \caption{1 Day Merged - F-Measure}
		      \label{fig:1day-merged-f-measure}
		  \end{figure}
		  
		  Similar results were registered for the 7-day data set. \\
		  The overall f-measure has increased from its baseline value, to a high of 0.36 for window values of 240 and 480 minutes.
		  Interestingly enough, the events detected by the evaluation script has increased as well, from 106 baseline to 112.
		  \\
		  Moreover, recall followed the trend set by the 1-day dataset and increased from 0.209 initially to 0.221 for window of 240 and 480 minutes respectively. 
		  The number of clusters has steadily decreased as the window grows, which was expected.
		  As a result, the precision measure of the system has grown as well.
	      \begin{figure}[h!]
		      \centering
		      \includegraphics[width=\linewidth]{images/7days-merged-f-measure.png}
		      \caption{7 Days Merged - F-Measure}
		      \label{fig:7days-merged-f-measure}
	      \end{figure}
\end{enumerate}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\section{Discussion}
\label{section-discussion}

The summary of the results can be seen in Figure \ref{fig:summary}.
The table shows the baseline results, along with the results with the highest f-measures of the two approaches applied on both data sets.\\
\\
From the results, there are a few interesting conclusions to draw. 
The first, is that the filtering step enhances the cluster identification step from McMinn's study \cite{McMinn2013} and greatly increases the evaluation results without any impact on the recall measure.
This can be said for both data sets.
As a result of the filtering process, the precision skyrocketed as well as the f-measure by an order of magnitude. \\
In terms of scalability, this step can be easily implemented on a stream of clusters as it is simply a case of pass/drop by comparing sizes.
However, one must be careful when choosing the variable to sort by.
Relevant clusters may easily be dropped if not handled carefully and thus may have a negative impact on recall.

\begin{figure}[h!]
	\centering
	\includegraphics[width=\linewidth]{images/summary.png}
	\caption{Summary of results}
	\label{fig:summary}
\end{figure}

By contrast, the merging approach did not attain the same highs results.
Still, the results were surprising since recall was not expected to increase further than the baseline.
This might be due the increase in the number of events detected.
It increased by a few values (by 1 in the 1-day dataset, by 6 in 7-day).
As a result, this might be considered an enhancement of the clustering step, where multiple clusters with the same named entity was created. 
It might be due to some users tweeting a number of minutes later than the event occurred.
\\\\
In theory, it is an unharmful operation if there are no other clusters with the same named entity in the same timed window.
Additionally, it would be interesting to find what the results are with the filtering step applied before the merge.
In theory, it should get rid of the small clusters with no relevance to an event, but as mentioned earlier, care must be taken with the variable on which to filter on.
\\
However, there are major setbacks to this approach.
The worst one is in the case where multiple events occurred in a period, all related to the same entity. 
This approach would fail at picking up both events. 

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\section{Traffic Event Detection}
\subsection{Problem Statement}
\subsection{Proposed Solution}
\subsection{Discussion}

%==============================================annotated bibliography
\newpage
\nocite{*}
\bibliographystyle{plain}
\bibliography{bibliography}
\pagebreak

% Activate the appendix
% from now on sections are numerated with capital letters
\appendix

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\section{Experiment Results}
\label{appendix:results}

\subsection{1-day Data-Set}
\subsubsection{Filtering}

\begin{figure}[H]
	\centering
	\includegraphics[width=\linewidth]{images/1day-filtered-events-detected.png}
	\caption{1-Day Filtered Events Detected}
	\label{fig:1day-filtered-events-detected}
\end{figure}

\begin{figure}[H]
	\centering
	\includegraphics[width=\linewidth]{images/1day-filtered-total-clusters.png}
	\caption{1-Day Filtered Total Clusters}
	\label{fig:1day-filtered-total-clusters}
\end{figure}

\begin{figure}[H]
	\centering
	\includegraphics[width=\linewidth]{images/1day-filtered-precision.png}
	\caption{1-Day Filtered Precision}
	\label{fig:1day-filtered-precision}
\end{figure}

\begin{figure}[]
	\centering
	\includegraphics[width=\linewidth]{images/1day-filtered-recall.png}
	\caption{1-Day Filtered Recall}
	\label{fig:1day-filtered-recall}
\end{figure}

\begin{figure}[H]
	\centering
	\includegraphics[width=\linewidth]{images/1day-filtered-f-measure.png}
	\caption{1-Day Filtered F-Measure}
	\label{fig:1day-filtered-f-measure}
\end{figure}

\subsubsection{Merging}

\begin{figure}[H]
	\centering
	\includegraphics[width=\linewidth]{images/1day-merged-events-detected.png}
	\caption{1-Day Merged Events Detected}
	\label{fig:1day-merged-events-detected}
\end{figure}

\begin{figure}[H]
	\centering
	\includegraphics[width=\linewidth]{images/1day-merged-total-clusters.png}
	\caption{1-Day Merged Total Clusters}
	\label{fig:1day-merged-total-clusters}
\end{figure}

\begin{figure}[H]
	\centering
	\includegraphics[width=\linewidth]{images/1day-merged-precision.png}
	\caption{1-Day Merged Precision}
	\label{fig:1day-merged-precision}
\end{figure}

\begin{figure}[H]
	\centering
	\includegraphics[width=\linewidth]{images/1day-merged-recall.png}
	\caption{1-Day Merged Recall}
	\label{fig:1day-merged-recall}
\end{figure}

\begin{figure}[H]
	\centering
	\includegraphics[width=\linewidth]{images/1day-merged-f-measure.png}
	\caption{1-Day Merged F-Measure}
	\label{fig:1day-merged-f-measure}
\end{figure}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\subsection{7-day Data-Set}
\subsubsection{Filtering}

\begin{figure}[H]
	\centering
	\includegraphics[width=\linewidth]{images/7days-filtered-events-detected.png}
	\caption{7-Day Filtered Events Detected}
	\label{fig:7day-filtered-events-detected}
\end{figure}

\begin{figure}[H]
	\centering
	\includegraphics[width=\linewidth]{images/7days-filtered-total-clusters.png}
	\caption{7-Day Filtered Total Clusters}
	\label{fig:7days-filtered-total-clusters}
\end{figure}

\begin{figure}[H]
	\centering
	\includegraphics[width=\linewidth]{images/7days-filtered-precision.png}
	\caption{7-Day Filtered Precision}
	\label{fig:7days-filtered-precision}
\end{figure}

\begin{figure}[H]
	\centering
	\includegraphics[width=\linewidth]{images/7days-filtered-recall.png}
	\caption{7-Day Filtered Recall}
	\label{fig:7days-filtered-recall}
\end{figure}

\begin{figure}[H]
	\centering
	\includegraphics[width=\linewidth]{images/7days-filtered-f-measure.png}
	\caption{7-Day Filtered F-Measure}
	\label{fig:7days-filtered-f-measure}
\end{figure}

\subsubsection{Merging}

\begin{figure}[H]
	\centering
	\includegraphics[width=\linewidth]{images/7days-merged-events-detected.png}
	\caption{1-Day Merged Events Detected}
	\label{fig:7days-merged-events-detected}
\end{figure}

\begin{figure}[H]
	\centering
	\includegraphics[width=\linewidth]{images/7days-merged-total-clusters.png}
	\caption{1-Day Merged Total Clusters}
	\label{fig:7days-merged-total-clusters}
\end{figure}

\begin{figure}[H]
	\centering
	\includegraphics[width=\linewidth]{images/7days-merged-precision.png}
	\caption{1-Day Merged Precision}
	\label{fig:7days-merged-precision}
\end{figure}

\begin{figure}[H]
	\centering
	\includegraphics[width=\linewidth]{images/7days-merged-recall.png}
	\caption{1-Day Merged Recall}
	\label{fig:7days-merged-recall}
\end{figure}

\begin{figure}[H]
	\centering
	\includegraphics[width=\linewidth]{images/7days-merged-f-measure.png}
	\caption{1-Day Merged F-Measure}
	\label{fig:7days-merged-f-measure}
\end{figure}


\end{document}
